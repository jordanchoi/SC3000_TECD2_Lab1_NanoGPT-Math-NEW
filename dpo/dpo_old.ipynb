{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {
    "id": "124a869a"
   },
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sLcgPb9WFNYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21063,
     "status": "ok",
     "timestamp": 1760969156676,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "sLcgPb9WFNYf",
    "outputId": "0f52df1b-6699-40c6-8b59-7e530d1c2b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f50312",
   "metadata": {},
   "source": [
    "#### Adjust based on your system requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b82f8f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8031,
     "status": "ok",
     "timestamp": 1760890457913,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "3b82f8f1",
    "outputId": "04e21e86-cbe6-4e10-9599-761bb512c752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (4.39.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (4.56.2)\n",
      "Requirement already satisfied: datasets in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (4.1.1)\n",
      "Requirement already satisfied: tiktoken in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: wandb in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: tqdm in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: pydantic<3 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (1.10.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.8.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tzi/.pyenv/versions/3.11.2/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "#%pip uninstall -y torch torchvision torchaudio\n",
    "#%pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {
    "id": "6c2d9de0"
   },
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528b978",
   "metadata": {},
   "source": [
    "#### Check for GPU in system (this was used as we were training on Google Colab so that we can use CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "NbTXSh70LV6i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1760974302547,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "NbTXSh70LV6i",
    "outputId": "7e0da8c4-d7b9-4692-efed-1e7c59fdcd4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 20 15:31:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   34C    P0             63W /  400W |    3093MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "IILc28EVLZNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1760974304749,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "IILc28EVLZNX",
    "outputId": "80405f9a-57e2-4162-e854-7532f2470671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m               dpo.ipynb          pos_neg_pairs.json\n",
      "dpo-delete.pt      dpo.pt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dd92d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1760974306008,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "876dd92d",
    "outputId": "486a782c-dd80-4588-cf7f-74b4f146732a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch cuda available: False\n",
      "device count: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 2e-4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.1\n",
    "top_k = None\n",
    "\n",
    "# Check the device being used\n",
    "print(device)\n",
    "print(\"torch cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    import subprocess\n",
    "    print(\"device name:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.randn(8192, 8192, device=\"cuda\")\n",
    "    y = x @ x\n",
    "    print(\"tensor on:\", y.device)\n",
    "    subprocess.run([\"nvidia-smi\"])\n",
    "\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {
    "id": "4c7d35e6"
   },
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03655c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760974308871,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "d03655c3"
   },
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss\n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {
    "id": "fc9d9eba"
   },
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceae772a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1760974310827,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "ceae772a",
    "outputId": "9a5da271-762e-48ed-a633-4eefab133637"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../sft/gpt.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ckpt = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../sft/gpt.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m gptconf = GPTConfig(**ckpt[\u001b[33m'\u001b[39m\u001b[33mmodel_args\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      3\u001b[39m gpt = GPT(gptconf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.2/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../sft/gpt.pt'"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {
    "id": "0feafc5a"
   },
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7edf3d44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2806,
     "status": "ok",
     "timestamp": 1760974315858,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "7edf3d44",
    "outputId": "b65d9e9a-4295-446f-d2a5-270eff0b3419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: 250000 pairs\n",
      "Positives passing numeric sanity: 250000/250000\n",
      "✅ Saved ./data/pos_neg_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "import random, json, re\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "TOTAL = 250000            # start small; scale after you confirm learning\n",
    "SEED = 20251020\n",
    "random.seed(SEED)\n",
    "\n",
    "# Ratios for Phase A (basic only)\n",
    "RATIOS = {\n",
    "    \"arithmetic_1step\": 0.60,\n",
    "    \"linear_eqn\":       0.40,\n",
    "\n",
    "    # keep these OFF for now (Phase B later)\n",
    "    \"expression_2steps\": 0.00,\n",
    "    \"edge_cases\":        0.00,\n",
    "}\n",
    "\n",
    "# Range limits (tight first)\n",
    "A_B_RANGE = (1, 50)     # operands for arithmetic\n",
    "X_K_B_RANGE = (1, 20)   # x, k, b for linear\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def gen_arithmetic():\n",
    "    a = random.randint(*A_B_RANGE)\n",
    "    b = random.randint(*A_B_RANGE)\n",
    "    op = random.choice(['+','-','*','/'])\n",
    "    if op == '/':\n",
    "        # choose b dividing a to keep integer result\n",
    "        divisors = [d for d in range(1, A_B_RANGE[1]+1) if a % d == 0]\n",
    "        b = random.choice(divisors) if divisors else 1\n",
    "        ans = a // b\n",
    "    elif op == '+': ans = a + b\n",
    "    elif op == '-': ans = a - b\n",
    "    else:           ans = a * b\n",
    "    prompt = f\"{a}{op}{b}=?\"\n",
    "    pos = f\"{prompt} The answer is {ans} because {a}{op}{b} equals {ans}.\"\n",
    "    # a plausible but wrong negative or a refusal\n",
    "    if random.random() < 0.5:\n",
    "        wrong = ans + random.choice([-3, -2, -1, 1, 2, 3])\n",
    "        neg = f\"{prompt} The answer is {wrong}.\"\n",
    "    else:\n",
    "        neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "def gen_linear():\n",
    "    # Three patterns: x+b=c, x-b=c, k*x=c\n",
    "    pat = random.choice([0,1,2])\n",
    "    if pat in (0,1):\n",
    "        x = random.randint(*X_K_B_RANGE)\n",
    "        b = random.randint(*X_K_B_RANGE)\n",
    "        if pat == 0:\n",
    "            c = x + b\n",
    "            prompt = f\"x+{b}={c},x=?\"\n",
    "        else:\n",
    "            c = x - b\n",
    "            prompt = f\"x-{b}={c},x=?\"\n",
    "        ans = x\n",
    "        pos = f\"{prompt} The answer is {ans} because {c}{'-' if pat==0 else '+'}{b} equals {ans}.\"\n",
    "    else:\n",
    "        k = random.randint(1, X_K_B_RANGE[1])\n",
    "        x = random.randint(*X_K_B_RANGE)\n",
    "        c = k * x\n",
    "        prompt = f\"{k}*x={c},x=?\"\n",
    "        ans = x\n",
    "        pos = f\"{prompt} The answer is {ans} because {c}/{k} equals {ans}.\"\n",
    "    # negatives\n",
    "    if random.random() < 0.5:\n",
    "        wrong = ans + random.choice([-3,-2,-1,1,2,3])\n",
    "        neg = f\"{prompt} The answer is {wrong}.\"\n",
    "    else:\n",
    "        neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# (Phase B) 2-steps & edge cases — keep disabled until base works\n",
    "def gen_expression_2steps():\n",
    "    a,b,c = [random.randint(1,9) for _ in range(3)]\n",
    "    op1, op2 = random.choice(['+','-','*']), random.choice(['+','-','*'])\n",
    "    prompt = f\"{a}{op1}({b}{op2}{c})=?\"\n",
    "    ans = eval(f\"{a}{op1}({b}{op2}{c})\")\n",
    "    pos = f\"{prompt} The answer is {ans}.\"\n",
    "    neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "def gen_edge_cases():\n",
    "    # simple negatives only; keep tight\n",
    "    a = random.randint(-20, 20)\n",
    "    b = random.randint(-20, 20)\n",
    "    op = random.choice(['+','-','*'])\n",
    "    prompt = f\"{a}{op}{b}=?\"\n",
    "    ans = eval(f\"{a}{op}{b}\")\n",
    "    pos = f\"{prompt} The answer is {ans}.\"\n",
    "    neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "GENS = {\n",
    "    \"arithmetic_1step\": gen_arithmetic,\n",
    "    \"linear_eqn\": gen_linear,\n",
    "    \"expression_2steps\": gen_expression_2steps,\n",
    "    \"edge_cases\": gen_edge_cases,\n",
    "}\n",
    "\n",
    "def build_dataset(total=TOTAL, ratios=RATIOS):\n",
    "    data = []\n",
    "    for k, r in ratios.items():\n",
    "        n = int(total * r)\n",
    "        if n <= 0: continue\n",
    "        for _ in range(n):\n",
    "            data.append(GENS[k]())\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "data = build_dataset()\n",
    "print(f\"Generated: {len(data)} pairs\")\n",
    "\n",
    "# ---------------- Validation ----------------\n",
    "arith_re = re.compile(r'^(-?\\d+)([+\\-*/])(-?\\d+)=\\?$')\n",
    "lin_add  = re.compile(r'^x\\+(-?\\d+)=(-?\\d+),x=\\?$')\n",
    "lin_sub  = re.compile(r'^x\\-(-?\\d+)=(-?\\d+),x=\\?$')\n",
    "lin_mul  = re.compile(r'^(\\d+)\\*x=(-?\\d+),x=\\?$')\n",
    "\n",
    "def extract_prompt(s):\n",
    "    # get the part before the first space\n",
    "    return s.split(' ', 1)[0]\n",
    "\n",
    "def check_positive_record(rec):\n",
    "    # Ensure the positive actually contains the correct number\n",
    "    s = rec[\"positive\"]\n",
    "    prompt = extract_prompt(s)\n",
    "\n",
    "    m = arith_re.match(prompt)\n",
    "    if m:\n",
    "        a, op, b = int(m.group(1)), m.group(2), int(m.group(3))\n",
    "        ans = a+b if op=='+' else a-b if op=='-' else a*b if op=='*' else (a//b)\n",
    "    else:\n",
    "        m1 = lin_add.match(prompt)\n",
    "        m2 = lin_sub.match(prompt)\n",
    "        m3 = lin_mul.match(prompt)\n",
    "        if m1:\n",
    "            b, c = int(m1.group(1)), int(m1.group(2))\n",
    "            ans = c - b\n",
    "        elif m2:\n",
    "            b, c = int(m2.group(1)), int(m2.group(2))\n",
    "            ans = c + b\n",
    "        elif m3:\n",
    "            k, c = int(m3.group(1)), int(m3.group(2))\n",
    "            ans = c // k\n",
    "        else:\n",
    "            # Other types are Phase B; skip strict check for now\n",
    "            return True\n",
    "\n",
    "    return f\" {ans}\" in s  # simple containment check for the number\n",
    "\n",
    "bad = [i for i,rec in enumerate(data) if not check_positive_record(rec)]\n",
    "print(f\"Positives passing numeric sanity: {len(data)-len(bad)}/{len(data)}\")\n",
    "if bad[:5]: print(\"First few suspicious indices:\", bad[:5])\n",
    "\n",
    "# ---------------- Save ----------------\n",
    "import os\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "path = \"./data/pos_neg_pairs.json\"\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(f\"✅ Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "t_DC5LEkOpuN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1760974317909,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "t_DC5LEkOpuN",
    "outputId": "fd9f29c2-7a09-4cd9-fdc7-2045fb687632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m               dpo.ipynb          pos_neg_pairs.json\n",
      "dpo-delete.pt      dpo.pt\n",
      "Loaded 250000 training pairs\n"
     ]
    }
   ],
   "source": [
    "# recommend to use the AdamW optimizer\n",
    "# Load data from ./data/pos_neg_pairs.json\n",
    "import json\n",
    "!ls\n",
    "path = \"./data/pos_neg_pairs.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(lines)} training pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {
    "id": "c2e5f81f"
   },
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0c400f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1760974326312,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "df0c400f",
    "outputId": "e840bc5f-9d08-49fc-ade6-1bcaf27b7838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and scheduler ready.\n"
     ]
    }
   ],
   "source": [
    "# recommend to use the AdamW optimizer\n",
    "\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "steps_per_epoch = max(1, len(lines) // batch_size)\n",
    "total_training_steps = steps_per_epoch * epochs\n",
    "\n",
    "warmup_iters = int(0.05 * total_training_steps)  # try 50% on third try, same warmup as run 2\n",
    "lr_decay_iters = total_training_steps\n",
    "\n",
    "min_lr = base_lr * 0.05 # try lower min_lr\n",
    "\n",
    "# === AdamW with proper decay ===\n",
    "decay, no_decay = [], []\n",
    "for n, p in gpt.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if n.endswith(\"bias\") or \"ln\" in n.lower() or \"layernorm\" in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "\n",
    "gpt.to(device)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": decay, \"weight_decay\": 0.1},\n",
    "     {\"params\": no_decay, \"weight_decay\": 0.0}],\n",
    "    lr=base_lr,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# cosine with warmup\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_iters:\n",
    "        return step / max(1, warmup_iters)\n",
    "    progress = (step - warmup_iters) / max(1, lr_decay_iters - warmup_iters)\n",
    "    return max(min_lr / base_lr, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "print(\"Optimizer and scheduler ready.\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../sft/meta.pkl\",\"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "# robust encode/decode: map any OOV char to PAD/NULL (prefer '\\x00' if it exists)\n",
    "PAD_ID = stoi.get('\\x00', 0)\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi.get(ch, PAD_ID) for ch in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {
    "id": "52b66199"
   },
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c1bc6",
   "metadata": {},
   "source": [
    "In order to train the model, we would need to compare the likelihood of a correct vs wrong answer through the model's mean sequence log-probability at each training step. \n",
    "\n",
    "From there, we can then apply a preference loss using the DPO technique. This helps the model learn by replicating preference patterns found in the comparison data we have provided by pushing it to score positives > negatives. \n",
    "\n",
    "This formula is denoted as follows : \n",
    "<p style=\"text-align:center;\">L<sub>pref</sub> ​= −E[ log σ( 1/β​( logp<sub>θ</sub>​(pos) − logp<sub>θ</sub>​(neg) ) ) ]</p>\n",
    "\n",
    "E refers to the expectation over the current training batch, and β refers to the temperature/margin scale (in this case it is inversed).\n",
    "\n",
    "For optimization, we use AdamW with stable learning-rate schedule. \n",
    "\n",
    "Each training step is: \n",
    "\n",
    "1. Zero the gradients from previous step \n",
    "2. Backpropagating the loss to compute the new gradients\n",
    "3. Gradient clipping the norm to be 1.0 or below to make it stable\n",
    "4. Use the gradients stored to update the parameters in AdamW\n",
    "5. Adjust the learning rate for the next step based on schedule\n",
    "\n",
    "As we are interested in the training curves, we recorded the loss per step and current LR. This is used for us to plot a graph later on and verify that the schedule and loss behave as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757999,
     "status": "ok",
     "timestamp": 1760976086603,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "1d4ebeb4",
    "outputId": "6d1c2944-77fe-40e5-c925-62a7a3cfd0fa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m''' # GIVEN\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mtotal_steps = len(lines) // batch_size\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mfor epoch in range(epochs):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[33;03m    print(f\"Saved checkpoint to {ckpt_path}\")\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m total_steps = \u001b[38;5;28mlen\u001b[39m(\u001b[43mlines\u001b[49m) // batch_size\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m#################################################\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Want to look at learning rate and loss n plot graph\u001b[39;00m\n\u001b[32m     27\u001b[39m loss_history = []\n",
      "\u001b[31mNameError\u001b[39m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "\n",
    "# Store the values so that we can look at learning rate and loss later in graph plot\n",
    "loss_history = []\n",
    "lr_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        '''\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples:\n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob\n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "        # ...\n",
    "        ###########################################################\n",
    "        '''\n",
    "\n",
    "        # Move tensors to device\n",
    "        neg_tensor = neg_tensor.to(device)\n",
    "        pos_tensor = pos_tensor.to(device)\n",
    "\n",
    "        # Compute log-probabilities with the helper provided in sample notebook\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "\n",
    "        # DPO preference loss: encourage pos > neg\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "\n",
    "        ### Want to look at learning rate and loss n plot graph\n",
    "        loss_val = loss.item()\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_history.append(current_lr)\n",
    "        ###\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Optionally record or display\n",
    "        loss_val = loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss_val})\n",
    "\n",
    "    ckpt_path = f\"dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498ccb3",
   "metadata": {},
   "source": [
    "With the loss history information, we are able to view the training loss as well as the learning rate per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "w31jzWWZPHEv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1760976117496,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "w31jzWWZPHEv",
    "outputId": "778f26b8-7f8e-4d46-c5f6-dfa82a7fd658"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Plot training loss per step\u001b[39;00m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m plt.plot(\u001b[43mloss_history\u001b[49m, label=\u001b[33m\"\u001b[39m\u001b[33mLoss per batch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mTraining step\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'loss_history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss per step\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_history, label=\"Loss per batch\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_history, label=\"Learning Rate\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate vs Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {
    "id": "48b7f2ab"
   },
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a032f0",
   "metadata": {},
   "source": [
    "#### Code Provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09027262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18670,
     "status": "ok",
     "timestamp": 1760976329753,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "09027262",
    "outputId": "d0fd2f1e-a10f-4992-8b52-fc7705f0e232"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "\n",
    "# Added 'cpu' as potential device as most of us are using Macbook Apple Silicon\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt = GPT(gptconf).to(device)\n",
    "\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"8+12=?\", \"50-23=?\", \"9*8=?\", \"81/9=?\", \"x+15=30,x=?\", \"7*x=49,x=?\", \"5*6=?\", \"100/25=?\", \"x-7=15,x=?\", \"9+x=20,x=?\", \"12*12=?\", \"64/8=?\", \"x+9=18,x=?\", \"10*10=?\", \"45/5=?\", \"x-4=10,x=?\", \"x+7=14,x=?\", \"6*7=?\", \"81/9=?\", \"x-3=12,x=?\", \"8+x=15,x=?\", \"14*14=?\", \"72/8=?\", \"x+8=16,x=?\", \"9-3=?\", \"15*3=?\", \"81/9=?\", \"x-5=10,x=?\", \"x+6=13,x=?\", \"11*11=?\", \"56/7=?\", \"x+10=20,x=?\", \"7*8=?\", \"90/9=?\", \"x-2=8,x=?\", \"x+5=12,x=?\", \"13*13=?\", \"72/6=?\", \"x-6=14,x=?\", \"4*9=?\", \"81/9=?\", \"x+4=10,x=?\", \"8*8=?\", \"60/5=?\", \"x-1=9,x=?\", \"x+3=11,x=?\", \"9*9=?\", \"49/7=?\", \"x-4=11,x=?\", \"6*6=?\", \"100/10=?\", \"x+2=12,x=?\", \"15-7=?\", \"8*7=?\", \"72/9=?\", \"x-8=12,x=?\", \"x+1=10,x=?\", \"14*12=?\", \"54/6=?\", \"x-9=11,x=?\", \"5*8=?\", \"81/9=?\", \"x+12=20,x=?\", \"7*9=?\", \"80/8=?\", \"x-7=13,x=?\", \"x+11=22,x=?\", \"12*11=?\", \"63/7=?\", \"x-5=15,x=?\", \"9*7=?\", \"90/10=?\", \"x+15=25,x=?\", \"6*8=?\", \"72/8=?\", \"x-4=16,x=?\", \"x+14=28,x=?\", \"11*12=?\", \"48/6=?\", \"x-3=13,x=?\", \"4*8=?\", \"56/8=?\", \"x+13=26,x=?\", \"10*12=?\", \"70/7=?\", \"x-6=18,x=?\", \"x+9=19,x=?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde8946",
   "metadata": {},
   "source": [
    "#### For Testing, we looped through the following steps : \n",
    "1. Encode the prompt\n",
    "2. Convert to tensor of token IDs\n",
    "3. Run through model\n",
    "4. Decode the output\n",
    "5. Print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ca7ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT : 17+19=?\n",
      "OUTPUT : 17+19=? The answer is 36 because 17+19 equals 36.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 3*17=?\n",
      "OUTPUT : 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/4=?\n",
      "OUTPUT : 72/4=? The answer is 7 because 72/4 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72-x=34,x=?\n",
      "OUTPUT : 72-x=34,x=? The answer is 16 because 34-7 equals 6.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x*11=44,x=?\n",
      "OUTPUT : x*11=44,x=? The answer is 4 because 44/11 equals 4.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 3*17=?\n",
      "OUTPUT : 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/4=?\n",
      "OUTPUT : 72/4=? The answer is 7 because 72/4 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72-x=34,x=?\n",
      "OUTPUT : 72-x=34,x=? The answer is 16 because 34-7 equals 6.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 8+12=?\n",
      "OUTPUT : 8+12=? The answer is 20 because 8+12 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 50-23=?\n",
      "OUTPUT : 50-23=? The answer is 27 because 50-23 equals 27.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 9*8=?\n",
      "OUTPUT : 9*8=? The answer is 72 because 9*8 equals 72.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+15=30,x=?\n",
      "OUTPUT : x+15=30,x=? The answer is 15 because 30-15 equals 15.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 7*x=49,x=?\n",
      "OUTPUT : 7*x=49,x=? The answer is 7 because 49/7 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 5*6=?\n",
      "OUTPUT : 5*6=? The answer is 30 because 5*6 equals 30.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 100/25=?\n",
      "OUTPUT : 100/25=? The answer is 5 because 10/12 equals 55.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-7=15,x=?\n",
      "OUTPUT : x-7=15,x=? The answer is 11 because 15+7 equals 11.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 9+x=20,x=?\n",
      "OUTPUT : 9+x=20,x=? The answer is 11 because 20-9 equals 11.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 12*12=?\n",
      "OUTPUT : 12*12=? The answer is 144 because 12*12 equals 144.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 64/8=?\n",
      "OUTPUT : 64/8=? The answer is 3 because 64/8 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+9=18,x=?\n",
      "OUTPUT : x+9=18,x=? The answer is 9 because 18-9 equals 9.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 10*10=?\n",
      "OUTPUT : 10*10=? The answer is 100 because 10*10 equals 100.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 45/5=?\n",
      "OUTPUT : 45/5=? The answer is 9 because 45/5 equals 9.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-4=10,x=?\n",
      "OUTPUT : x-4=10,x=? The answer is 14 because 10+4 equals 14.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+7=14,x=?\n",
      "OUTPUT : x+7=14,x=? The answer is 7 because 14-7 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 6*7=?\n",
      "OUTPUT : 6*7=? The answer is 42 because 6*7 equals 42.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-3=12,x=?\n",
      "OUTPUT : x-3=12,x=? The answer is 15 because 12+3 equals 15.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 8+x=15,x=?\n",
      "OUTPUT : 8+x=15,x=? The answer is 7 because 15-8 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 14*14=?\n",
      "OUTPUT : 14*14=? The answer is 196 because 14*14 equals 196.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/8=?\n",
      "OUTPUT : 72/8=? The answer is 4 because 72/8 equals 4.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+8=16,x=?\n",
      "OUTPUT : x+8=16,x=? The answer is 8 because 16-8 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 9-3=?\n",
      "OUTPUT : 9-3=? The answer is 6 because 9-3 equals 6.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 15*3=?\n",
      "OUTPUT : 15*3=? The answer is 45 because 15*3 equals 45.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-5=10,x=?\n",
      "OUTPUT : x-5=10,x=? The answer is 15 because 10+5 equals 15.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+6=13,x=?\n",
      "OUTPUT : x+6=13,x=? The answer is 7 because 13-6 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 11*11=?\n",
      "OUTPUT : 11*11=? The answer is 121 because 11*11 equals 121.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 56/7=?\n",
      "OUTPUT : 56/7=? The answer is 8 because 56/7 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+10=20,x=?\n",
      "OUTPUT : x+10=20,x=? The answer is 10 because 20-10 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 7*8=?\n",
      "OUTPUT : 7*8=? The answer is 56 because 7*8 equals 56.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 90/9=?\n",
      "OUTPUT : 90/9=? The answer is 2 because 90/9 equals 2.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-2=8,x=?\n",
      "OUTPUT : x-2=8,x=? The answer is 10 because 8+2 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+5=12,x=?\n",
      "OUTPUT : x+5=12,x=? The answer is 7 because 12-5 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 13*13=?\n",
      "OUTPUT : 13*13=? The answer is 169 because 13*13 equals 169.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/6=?\n",
      "OUTPUT : 72/6=? The answer is 2 because 72/6 equals 2.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-6=14,x=?\n",
      "OUTPUT : x-6=14,x=? The answer is 20 because 14+6 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 4*9=?\n",
      "OUTPUT : 4*9=? The answer is 36 because 4*9 equals 36.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+4=10,x=?\n",
      "OUTPUT : x+4=10,x=? The answer is 6 because 10-4 equals 6.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 8*8=?\n",
      "OUTPUT : 8*8=? The answer is 64 because 8*8 equals 64.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 60/5=?\n",
      "OUTPUT : 60/5=? The answer is 10 because 60/5 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-1=9,x=?\n",
      "OUTPUT : x-1=9,x=? The answer is 10 because 9+1 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+3=11,x=?\n",
      "OUTPUT : x+3=11,x=? The answer is 8 because 11-3 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 9*9=?\n",
      "OUTPUT : 9*9=? The answer is 81 because 9*9 equals 81.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 49/7=?\n",
      "OUTPUT : 49/7=? The answer is 7 because 49/7 equals 7.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-4=11,x=?\n",
      "OUTPUT : x-4=11,x=? The answer is 15 because 11+4 equals 15.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 6*6=?\n",
      "OUTPUT : 6*6=? The answer is 36 because 6*6 equals 36.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 100/10=?\n",
      "OUTPUT : 100/10=? The answer is 10 because 10/11 equals 1.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+2=12,x=?\n",
      "OUTPUT : x+2=12,x=? The answer is 10 because 12-2 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 15-7=?\n",
      "OUTPUT : 15-7=? The answer is 8 because 15-7 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 8*7=?\n",
      "OUTPUT : 8*7=? The answer is 56 because 8*7 equals 56.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/9=?\n",
      "OUTPUT : 72/9=? The answer is 2 because 72/9 equals 2.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-8=12,x=?\n",
      "OUTPUT : x-8=12,x=? The answer is 20 because 12+8 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+1=10,x=?\n",
      "OUTPUT : x+1=10,x=? The answer is 9 because 10-1 equals 9.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 14*12=?\n",
      "OUTPUT : 14*12=? The answer is 168 because 14*12 equals 168.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 54/6=?\n",
      "OUTPUT : 54/6=? The answer is 4 because 54/6 equals 4.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-9=11,x=?\n",
      "OUTPUT : x-9=11,x=? The answer is 20 because 11+9 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 5*8=?\n",
      "OUTPUT : 5*8=? The answer is 40 because 5*8 equals 40.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+12=20,x=?\n",
      "OUTPUT : x+12=20,x=? The answer is 8 because 20-12 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 7*9=?\n",
      "OUTPUT : 7*9=? The answer is 63 because 7*9 equals 63.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 80/8=?\n",
      "OUTPUT : 80/8=? The answer is 5 because 80/8 equals 5.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-7=13,x=?\n",
      "OUTPUT : x-7=13,x=? The answer is 20 because 13+7 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+11=22,x=?\n",
      "OUTPUT : x+11=22,x=? The answer is 11 because 22-11 equals 11.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 12*11=?\n",
      "OUTPUT : 12*11=? The answer is 132 because 12*11 equals 132.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 63/7=?\n",
      "OUTPUT : 63/7=? The answer is 3 because 63/7 equals 3.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-5=15,x=?\n",
      "OUTPUT : x-5=15,x=? The answer is 20 because 15+5 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 9*7=?\n",
      "OUTPUT : 9*7=? The answer is 63 because 9*7 equals 63.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 90/10=?\n",
      "OUTPUT : 90/10=? The answer is 9 because 90/10 equals 9.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+15=25,x=?\n",
      "OUTPUT : x+15=25,x=? The answer is 10 because 25-15 equals 10.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 6*8=?\n",
      "OUTPUT : 6*8=? The answer is 48 because 6*8 equals 48.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 72/8=?\n",
      "OUTPUT : 72/8=? The answer is 4 because 72/8 equals 4.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-4=16,x=?\n",
      "OUTPUT : x-4=16,x=? The answer is 20 because 16+4 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+14=28,x=?\n",
      "OUTPUT : x+14=28,x=? The answer is 14 because 28-14 equals 14.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 11*12=?\n",
      "OUTPUT : 11*12=? The answer is 132 because 11*12 equals 132.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 48/6=?\n",
      "OUTPUT : 48/6=? The answer is 8 because 48/6 equals 8.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-3=13,x=?\n",
      "OUTPUT : x-3=13,x=? The answer is 16 because 13+3 equals 16.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 4*8=?\n",
      "OUTPUT : 4*8=? The answer is 32 because 4*8 equals 32.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 56/8=?\n",
      "OUTPUT : 56/8=? The answer is 2 because 56/8 equals 2.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+13=26,x=?\n",
      "OUTPUT : x+13=26,x=? The answer is 13 because 26-13 equals 13.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 10*12=?\n",
      "OUTPUT : 10*12=? The answer is 120 because 10*12 equals 120.\n",
      "------------------------------------------------------------\n",
      "PROMPT : 70/7=?\n",
      "OUTPUT : 70/7=? The answer is 2 because 70/7 equals 2.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x-6=18,x=?\n",
      "OUTPUT : x-6=18,x=? The answer is 20 because 18+6 equals 20.\n",
      "------------------------------------------------------------\n",
      "PROMPT : x+9=19,x=?\n",
      "OUTPUT : x+9=19,x=? The answer is 10 because 19-9 equals 10.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        '''\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        '''\n",
    "\n",
    "        # Converts the prompt to a tensor of token IDs based on our tokeniser\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, :]\n",
    "        \n",
    "        # Run through the trained model to generate the resultant tensor values\n",
    "        y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature,top_k=top_k)\n",
    "\n",
    "        # We will take the tensor of token IDs and flatten it out to prep it for decoding\n",
    "        ans = y[0].reshape(-1).tolist()\n",
    "\n",
    "        # The output will be the decoded token IDs\n",
    "        output = decode(ans)\n",
    "\n",
    "        # Print out the prompt and output respectively\n",
    "        print(f\"PROMPT : {prompt}\")\n",
    "        print(f\"OUTPUT : {output}\")\n",
    "        print(\"-\" * 60)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f55bb5",
   "metadata": {},
   "source": [
    "#### For quick testing with text file, follow the steps and just run the cell below\n",
    "1. Ensure the test case are in a .txt file in the following format :<br>\n",
    "[\"17+19=?\", \"3*17=?\", ... ,\"72/4=?\"]\n",
    "2. Edit the string_path variable\n",
    "3. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe7a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : File not found\n"
     ]
    }
   ],
   "source": [
    "# To be modified\n",
    "string_path = \"\" \n",
    "\n",
    "# No editing required\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "max_new_tokens = 200\n",
    "temperature = 0.05\n",
    "top_k = None\n",
    "\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt_path = \"dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.eval()\n",
    "\n",
    "try:\n",
    "    with open(string_path, \"r\") as file:\n",
    "        priv_set = file.read()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for prompt in priv_set:\n",
    "            prompt_ids = encode(prompt)\n",
    "            x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, :]\n",
    "            y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature,top_k=top_k)\n",
    "            ans = y[0].reshape(-1).tolist()\n",
    "            output = decode(ans)\n",
    "\n",
    "            print(f\"PROMPT : {prompt}\")\n",
    "            print(f\"OUTPUT : {output}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : File not found\")\n",
    "except IOError as e:\n",
    "    print(f\"Error : {e}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
