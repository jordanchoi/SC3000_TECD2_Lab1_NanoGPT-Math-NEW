{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01edb185",
   "metadata": {},
   "source": [
    "## <u>Contributions :</u>\n",
    "\n",
    "Mabel Choi Jingyi\n",
    "- Generate postive-negative pairs\n",
    "- Building Data Loading Codes\n",
    "- Building Optimiser & Scheduler Codes\n",
    "- Formatting of Jupyter Notebook\n",
    "\n",
    "Choi Shu Yih, Jordan\n",
    "- Generate postive-negative pairs\n",
    "- Building Model Training Codes\n",
    "- Optimising and finetuning hyperparamters\n",
    "- Training the model\n",
    "\n",
    "Koh Tzi Yang\n",
    "- Generate postive-negative pairs\n",
    "- Building Test Codes\n",
    "- Formatting of Jupyter Notebook\n",
    "- Rerunning code cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {
    "id": "124a869a"
   },
   "source": [
    "## <u>Step 1: Install Necesscary Packages and Setting up Environment</u>\n",
    "This step (installing necessary packages) is required as this will ensure that all required and relevant Python Libraries and Packages are ready and installed in the environment for running the DPO training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98343b83",
   "metadata": {},
   "source": [
    "### <u>Step 1.1: Allow Google Drive Access to Google Collab Environment</u>\n",
    "\n",
    "Since our own computational resources are not good enough for the training of this NanoGPT model, we made use of Google Collab, which offers free access to powerful hardware resources and real-time collaboration, which are crucial for accelerating the task of training the NanoGPT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sLcgPb9WFNYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21063,
     "status": "ok",
     "timestamp": 1760969156676,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "sLcgPb9WFNYf",
    "outputId": "0f52df1b-6699-40c6-8b59-7e530d1c2b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "### Mount Google Drive to the Google Collab environment ###\n",
    "\n",
    "# This step is necessary as it will ensure that the notebook has read and write access to the \\\n",
    "# Google Drive containing the project's assets\n",
    "# - Load the dataset file and the pre-trained NanoGPT weights and checkpoints\n",
    "# - Save and reload the DPO's fine-tuned checkpoints (dpo.pt with model weights)\n",
    "\n",
    "# Successful mounting will resulted in a \"Mounted at /content/drive\" output\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e8f4e",
   "metadata": {},
   "source": [
    "### <u>Step 1.2: Install Relevant Packages</u>\n",
    "\n",
    "For this step, we will be installing the necessary Python Packages that will be able to allow us to achieve different tasks:\n",
    "- _**matplotlib**_: For data visualisation and plotting the training metrics\n",
    "- _**torch**_: PyTorch's deep learning framework for model training\n",
    "- _**numpy**_: Numerical computing library\n",
    "- _**transformers**_: Used for tokenisation utilities\n",
    "- _**datasets**_: Used for data loading and processing\n",
    "- _**tiktoken**_: Tokenisation library\n",
    "- _**wandb**_: Weights and biases, used for experiment tracking\n",
    "- _**tqdm**_: Progress bar library, used for monitoring training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82f8f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8031,
     "status": "ok",
     "timestamp": 1760890457913,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "3b82f8f1",
    "outputId": "04e21e86-cbe6-4e10-9599-761bb512c752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm\n",
    "#%pip uninstall -y torch torchvision torchaudio\n",
    "#%pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {
    "id": "6c2d9de0"
   },
   "source": [
    "## <u>Step 2: Package Imports and Configuration</u>\n",
    "\n",
    "This step (Package Imports and Configuration) is necessary as loading the packages to be used is required for usage of the packages. Additionally, setting up and configurating the hyperparameters were also necessary for the DPO training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400faf5",
   "metadata": {},
   "source": [
    "### <u>Step 2.1: Confirming Configuration</u>\n",
    "\n",
    "This step is necessary as we would need to confirm that we have the necessary computational power, and make sure that we are in the correct working directory and make sure that we have all the necessary packages imported before we can start with the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NbTXSh70LV6i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1760974302547,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "NbTXSh70LV6i",
    "outputId": "7e0da8c4-d7b9-4692-efed-1e7c59fdcd4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 20 15:31:42 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   34C    P0             63W /  400W |    3093MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "### GPU Verification ###\n",
    "# This step will allow us to display and view information about the available GPU\n",
    "# - Information regarding the GPU's model and driver version, etc\n",
    "# - This will also let us know if Google Collab is able to provide us with the computational power \\\n",
    "# needed for the NanoGPT model training\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3413c68",
   "metadata": {},
   "source": [
    "Based on the output that was returned above, from querying for the resources available in the Google Collab notebook, we can confirm that a Nvidia GPU with CUDA 12.4 is available, thus, providing the sufficient computational power that we need for training the NanoGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IILc28EVLZNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1760974304749,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "IILc28EVLZNX",
    "outputId": "80405f9a-57e2-4162-e854-7532f2470671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configurator.py  data  dpo  model.py  __pycache__  README.md  sft\n",
      "/content/drive/MyDrive/Colab Notebooks/Lab-1 NEW/SC3000_TECD2_Lab1_NanoGPT-Math-NEW\n",
      "configurator.py  data  dpo  model.py  __pycache__  README.md  sft\n"
     ]
    }
   ],
   "source": [
    "### Change Directory ###\n",
    "# - This step will also list the contents, thus, allowing us to confirm we are in the correct \\\n",
    "# and relevant directory so the relative paths can resolve. (eg. 'sft/gpt.pt')\n",
    "# - Before proceeding with the actual training portion of the DPO model, we would need to \\\n",
    "# change directory to the folder that contains our dataset and pre-trained NanoGPT model\n",
    "\n",
    "!ls\n",
    "%cd /content/drive/MyDrive/Colab Notebooks/Lab-1 NEW/SC3000_TECD2_Lab1_NanoGPT-Math-NEW\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46500ae0",
   "metadata": {},
   "source": [
    "Based on the above output that was returned, we can confirm that we are in the correct working directory and we can continue to the next step on importing the required necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c042afb",
   "metadata": {},
   "source": [
    "### <u>Step 2.2: Importing required necessary packages, Hyperparameter Configuration, and Tokeniser Loading</u>\n",
    "\n",
    "#### Step 2.2.1: Importing Required Packages\n",
    "For the first part of this step, we will need to import the relevant necessary packages to achieve different tasks:\n",
    "- _**PyTorch modules**_: Required for neural network operations\n",
    "- _**Pickle and JSON**_: For data serialisation\n",
    "- _**GPT and GPTConfig**_: Classes from model.py\n",
    "- _**tqdm**_: Progress bar library, used for monitoring training loops\n",
    "- _**matplotlib**_: For data visualisation and plotting the training metrics\n",
    "\n",
    "#### Step 2.2.2: Hyperparameter Configuration\n",
    "This contains the fine-tuning data for the DPO model.\n",
    "- _**beta**_: This is the DPO temperature parameter that controls the strength of preference learning. For beta values, the higher the value the more conservative the learning, whereas, the lower the value the more aggressive the learning.\n",
    "    - For us, we set it to 0.5, as it will be able to provide a good balance for the learning whereby it is aggressive enough to learn the preferences effectively but at the same time it is also not so aggressive whereby it will cause training instability.\n",
    "- _**base lr**_: This is the learning rate that is for the AdamW optimiser\n",
    "    - For us, we set it to 2e-4 = 0.0002, which allows for faster convergence during the DPO model training. Additionally, since we are not completely retraining the model, this learning rate enables efficient adaptation without catastrophic forgetting of the pre-trained weights and knowledge.\n",
    "- _**epochs**_: This refers to the number of complete passing through of the dataset. \n",
    "    - For us, we set the number of epochs to 10 as it will then provide a sufficient training time for the NanoGPT model to learn mathematical reasoning patterns.\n",
    "    - Fewer epochs, such as 3 epochs, might result in incomplete training for the NanoGPT model, while more epochs, such as 20 epochs, might result in overfitting, where the model will memorise the examples rather than generalising it.\n",
    "- _**batch size**_: This refers to the amount of pos-neg data pairs that are processed simultaneously per gradient update.\n",
    "    - For us, we chose this batch size to maximise the GPU's memory utilisation without any overflow.\n",
    "    - This batch size ensures consistent batch statistics across training, and provides a stable gradient estimates, as larger batches might give a noisier gradients.\n",
    "- _**max length**_: This refers to the maximum sequence length for the input sequences. This is because most of our mathematical problems are within the length we have set (64), thus, allowing for efficient batched processing.\n",
    "    - Essentially, every one of our sequence will be set to max 64 in length, whereby the sequences longer than 64 will be truncated, whereas the shorter sequences will be padded with zeros.\n",
    "- _**num samples**_: This refers to the number of response samples to geenrate per prompt during the model's evaluation.\n",
    "    - For us, we set this value to 1, for efficiency during the testing phase.\n",
    "- _**max new tokens**_: This refers to the maximum number of tokens to be generated during inference.\n",
    "    - For us, we set it to 200, as this limit ensures complete answers with explanations included, while preventing infinite loops generation at the same time. \n",
    "- _**temperature**_: This controls the randomness in the text generations.\n",
    "    - Lower temperatures are usually used for deterministic problems, whereby the most likely token is preferred.\n",
    "    - Higher temperatures are usually used for when diverysity is desired, such as creative writing.\n",
    "    - For us, we set the temperature to be 0.1, which is a very low temperature, thus, making the generation highly deterministic and focused. For mathematical problems, we are looking for precise and correct answers, thus, lower temperatures would ensure that the model consistently geenrates the most probable and correct responses.\n",
    "- _**top k**_: This refers to how much of the tokens in the vocabulary are or will be considered during generation. \n",
    "    - For us, we disabled top k sampling, so all of the tokens in the vocabulary are considered during generation.\n",
    "    - For math problems, there is no need to artifically restrict the vocabulary, and together with the low temperature, it already guarantee more or less a focused generation. Thus, we set it to None, so that the model can have full flexibility in choosing the correct mathematical terms, numbers and words used for explanations.\n",
    "\n",
    "#### Step 2.2.3: Tokeniser Loading\n",
    "This tokeniser will load the character-level vocabulary from the pre-trained NanoGPT Model.\n",
    "- _**stoi**_: maps the characters to token IDs\n",
    "- _**itos**_: maps token IDs back to characters\n",
    "- _**encode**_: converts text strings to token ID sequences\n",
    "- _**decode**_: converts token ID sequences back to text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dd92d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1760974306008,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "876dd92d",
    "outputId": "486a782c-dd80-4588-cf7f-74b4f146732a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch cuda available: True\n",
      "device count: 1\n",
      "device name: NVIDIA A100-SXM4-80GB\n",
      "tensor on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "### Step 2.2.1 - Importing Required Packages ###\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from model import GPT, GPTConfig\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Step 2.2.2 - Hyperparameter Configuration ###\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 2e-4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.1\n",
    "top_k = None\n",
    "\n",
    "\n",
    "### Checking Available Resource ###\n",
    "# check that there is GPU available and can be used for training in the later step\n",
    "print(device)\n",
    "print(\"torch cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    import subprocess\n",
    "    print(\"device name:\", torch.cuda.get_device_name(0))\n",
    "    x = torch.randn(8192, 8192, device=\"cuda\")\n",
    "    y = x @ x\n",
    "    print(\"tensor on:\", y.device)\n",
    "    subprocess.run([\"nvidia-smi\"])\n",
    "\n",
    "\n",
    "### Step 2.2.3 - Tokeniser Loading ###\n",
    "with open(\"sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {
    "id": "4c7d35e6"
   },
   "source": [
    "## <u>Step 3: Define helper functions</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d03655c3",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760974308871,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "d03655c3"
   },
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss\n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {
    "id": "fc9d9eba"
   },
   "source": [
    "## <u>Step 4: Load the pretrained NanoGPT model</u>\n",
    "\n",
    "This step will load the pre-trained NanoGPT model that will serve as the starting point for the DPO model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ceae772a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1760974310827,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "ceae772a",
    "outputId": "9a5da271-762e-48ed-a633-4eefab133637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {
    "id": "0feafc5a"
   },
   "source": [
    "## <u>Step 5: Load Data (**students are required to complete this part!**)</u>\n",
    "\n",
    "In this step, we will generate the dataset consisting of the positive and negative datapairs, and load the dataset as well, which will be used to train the DPO model.\n",
    "\n",
    "This step is crucial as it is the foundation for the DPO model training. We would need to create a large-scale dataset of positive and negative data pairs whereby the positive examples represent the correct mathematical solutions to mathematics problems with explanations, and negative examples represent incorrect or confused response to mathematics problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70d94b",
   "metadata": {},
   "source": [
    "### <u>Step 5.1: Dataset Generation</u>\n",
    "\n",
    "For our _**generation of the positive and negative data pairs dataset**_, we initially created a **2-phased approach**.\n",
    "- _**Phase 1**_ would focus on the basic arithmetic and simple linear equations to come up with basic foundational mathematical reasonings\n",
    "- Then _**Phase 2**_ would gradually introduce more complex arithmetic operations\n",
    "\n",
    "This hopefully would be able to help the model to learn basic fundemental mathematical arithmetic patterns first, before tackling the more complex operations; similar to how humans learn math.\n",
    "\n",
    "However, for our final dataset that we used, we decided to turn off the phase 2 dataset generation as the model seemed to learn and perform better with the easier dataset.\n",
    "\n",
    "In this step, we will programmatically build a data pair dataset consisting of 250,000 pairs of positive and negative math data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf3d44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2806,
     "status": "ok",
     "timestamp": 1760974315858,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "7edf3d44",
    "outputId": "b65d9e9a-4295-446f-d2a5-270eff0b3419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: 250000 pairs\n",
      "Positives passing numeric sanity: 250000/250000\n",
      "Saved ./data/pos_neg_pairs.json\n"
     ]
    }
   ],
   "source": [
    "# Data Pairs Generation\n",
    "# Ensure packages imported\n",
    "import random, json, re\n",
    "\n",
    "\n",
    "### Configuration ###\n",
    "# We generate 250,000 training pairs as this would be large enough for robust learning and generalisation\n",
    "TOTAL = 250000\n",
    "# Seed is used for the reproducibility of datasets\n",
    "SEED = 20251020\n",
    "# Ratios for different Phases\n",
    "RATIOS = {\n",
    "    # Phase 1 ratios\n",
    "    \"arithmetic_1step\": 0.60, # addotion, subtraction, multiplication, division\n",
    "    \"linear_eqn\":       0.40, # solving for x in simple equations\n",
    "\n",
    "    # Phase 2 ratios - turned off due to complexity and model's inability to learn from these expressions\n",
    "    \"expression_2steps\": 0.00,\n",
    "    \"edge_cases\":        0.00,\n",
    "}\n",
    "# Range limits for operands used in dataset\n",
    "A_B_RANGE = (1, 50)     # operands for arithmetic operations - range is small enough to avoid large number challenges and large enough to ensure generalisation, small ranges also helped the model to learn patterns quickly\n",
    "X_K_B_RANGE = (1, 20)   # operands for linear equations - range is small enough to keep algebric problems manageable and prevents complex multi-digit calculations\n",
    "\n",
    "\n",
    "### Helper Functions ###\n",
    "# Arithmetic Generation Function (Phase 1)\n",
    "def gen_arithmetic():\n",
    "    a = random.randint(*A_B_RANGE) # get random operand a\n",
    "    b = random.randint(*A_B_RANGE) # get random operand b\n",
    "    op = random.choice(['+','-','*','/']) # get random operator for a and b; equal probability\n",
    "    \n",
    "    # generate problems\n",
    "    if op == '/':\n",
    "        # only generate division problems with integer results - tokeniser might struggle with decimals\n",
    "        divisors = [d for d in range(1, A_B_RANGE[1]+1) if a % d == 0] # find all divisors of a, and randomly select one of the divisors as b\n",
    "        b = random.choice(divisors) if divisors else 1 # if there is no divisor of a, default to b = 1\n",
    "        ans = a // b # ensure whole number results\n",
    "    elif op == '+': ans = a + b\n",
    "    elif op == '-': ans = a - b\n",
    "    else:           ans = a * b\n",
    "    \n",
    "    # create prompt structure as a standard math question format\n",
    "    prompt = f\"{a}{op}{b}=?\"\n",
    "    # generate positive result with answer from previous section and form as a structured output - model will learn to give reasoning as well during model testing\n",
    "    pos = f\"{prompt} The answer is {ans} because {a}{op}{b} equals {ans}.\"\n",
    "    # generate negative result with wrong answer\n",
    "    if random.random() < 0.5: # based on randomness, generate common miscalculations - stimulates common calculation errors\n",
    "        wrong = ans + random.choice([-3, -2, -1, 1, 2, 3])\n",
    "        neg = f\"{prompt} The answer is {wrong}.\"\n",
    "    else: # still generate a response instead of giving up - like the pre-trained NanoGPT model\n",
    "        neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    \n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# Linear Equation Generation Function (Phase 1)\n",
    "def gen_linear():\n",
    "    # three patterns for linear equations: x+b=c, x-b=c, x*k=c\n",
    "    pat_choice = random.choice([0,1,2]) # get a random choice of pattern; equal probability\n",
    "\n",
    "    # generate problems with positive results\n",
    "    if pat_choice in (0,1): # if pattern choice is 0 or 1, will be a + or - operation\n",
    "        x = random.randint(*X_K_B_RANGE) # get random operand x (ans)\n",
    "        b = random.randint(*X_K_B_RANGE) # get random operand b\n",
    "        if pat_choice == 0: # addition linear equation if choice is 0\n",
    "            c = x + b\n",
    "            prompt = f\"x+{b}={c},x=?\"\n",
    "        else: # subtraction linear equation if choice is 1\n",
    "            c = x - b\n",
    "            prompt = f\"x-{b}={c},x=?\"\n",
    "        ans = x\n",
    "        # generate positive result with answer from previous section and form as a structured output - model will learn to give reasoning as well during model testing\n",
    "        pos = f\"{prompt} The answer is {ans} because {c}{'-' if pat_choice==0 else '+'}{b} equals {ans}.\" # inverse + and -, to find the answer\n",
    "    else: # else it will be a multiplication linear equation if choice is 2\n",
    "        k = random.randint(1, X_K_B_RANGE[1]) # get random operand k\n",
    "        x = random.randint(*X_K_B_RANGE) # get random operand x (ans)\n",
    "        c = k * x\n",
    "        prompt = f\"{k}*x={c},x=?\"\n",
    "        ans = x\n",
    "        # generate positive result with answer from previous section and form as a structured output - model will learn to give reasoning as well during model testing\n",
    "        pos = f\"{prompt} The answer is {ans} because {c}/{k} equals {ans}.\"\n",
    "    \n",
    "    # generate negative result with wrong answer\n",
    "    if random.random() < 0.5: # based on randomness, generate common miscalculations - stimulates common calculation errors\n",
    "        wrong = ans + random.choice([-3,-2,-1,1,2,3])\n",
    "        neg = f\"{prompt} The answer is {wrong}.\"\n",
    "    else: # still generate a response instead of giving up - like the pre-trained NanoGPT model\n",
    "        neg = f\"{prompt} Sorry, I do not know!\"\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# Phase 2 - This phase will NOT BE USED in our FINAL DATASET as we realised that the model learns better with the basic fundemental mathematical arithmetic patterns\n",
    "# However, we will still go through our logic in implementing these functions\n",
    "# The implementation is not as in-depth as our 'Phase 1' definition implementations as we originally only came up with data pair datasets that gives the correct answers for positive and \"Sorry, I do not know!\" for negative\n",
    "# The implementation for the 'Phase 1' was more in-depth as we wanted to test out the model's capability to learn common errors and not make the same errors\n",
    "# Since the model cannot seem to learn as well from these complex equations, we did not implement more in-depth codes for the 'Phase 2' definition functions\n",
    "# 2 Steps Expression Generation\n",
    "def gen_expression_2steps(): # this function creates expressions that requires knowledge of order of mathematical operations (eg. BODMAS)\n",
    "    a,b,c = [random.randint(1,9) for _ in range(3)] # generate random operands\n",
    "    op1, op2 = random.choice(['+','-','*']), random.choice(['+','-','*']) # generate random operators\n",
    "    prompt = f\"{a}{op1}({b}{op2}{c})=?\" # generate prompts that requires knowledge of order of mathematical operations (eg. BODMAS)\n",
    "    ans = eval(f\"{a}{op1}({b}{op2}{c})\") # evaluate answer with Python's built in function\n",
    "    pos = f\"{prompt} The answer is {ans}.\" # generate positive result\n",
    "    neg = f\"{prompt} Sorry, I do not know!\" # generate negative result\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "# Edge Cases Generation\n",
    "def gen_edge_cases():\n",
    "    # simple negatives only; range is kept small to keep problems manageable\n",
    "    a = random.randint(-20, 20) # generate random operand a\n",
    "    b = random.randint(-20, 20) # generate random operand b\n",
    "    op = random.choice(['+','-','*']) # generate random operator\n",
    "    prompt = f\"{a}{op}{b}=?\" # generate prompt based on random operands and operator, without doing checking if it goes to negative or is 0\n",
    "    ans = eval(f\"{a}{op}{b}\") # evaluate answer with Python's built in function\n",
    "    pos = f\"{prompt} The answer is {ans}.\" # generate positive result\n",
    "    neg = f\"{prompt} Sorry, I do not know!\" # generate negative result\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "# Dataset Building Functions\n",
    "GENS = { \n",
    "    # as named in our ratios for different phases \\\n",
    "    # thus maps category names to generator functions\n",
    "    \"arithmetic_1step\": gen_arithmetic,\n",
    "    \"linear_eqn\": gen_linear,\n",
    "    \"expression_2steps\": gen_expression_2steps,\n",
    "    \"edge_cases\": gen_edge_cases,\n",
    "}\n",
    "def build_dataset(total=TOTAL, ratios=RATIOS):\n",
    "    data = []\n",
    "    # iterate through each of the categories in the different phases and ratios, and calculate total number of examples\n",
    "    for k, r in ratios.items():\n",
    "        n = int(total * r) # set the total number of example to variable n\n",
    "        if n <= 0: continue # if there is no ratio allocated, skip\n",
    "        # append each generated pair to the appropriate generator function, n times\n",
    "        for _ in range(n):\n",
    "            data.append(GENS[k]())\n",
    "    \n",
    "    # then randomise the order of all generated pair (no longer located with same problem types)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "# Build the final dataset\n",
    "data = build_dataset()\n",
    "print(f\"Generated: {len(data)} pairs\") # print to check dataset generated correctly\n",
    "\n",
    "\n",
    "### Validation ###\n",
    "# This is done to catch any possible generation bugs during the generation of the data pair dataset\n",
    "# This is done with regular expression pattern matching\n",
    "arith_re = re.compile(r'^(-?\\d+)([+\\-*/])(-?\\d+)=\\?$') # to check if it matches \"number operator number = ?\"\n",
    "lin_add  = re.compile(r'^x\\+(-?\\d+)=(-?\\d+),x=\\?$') # to check if it matches \"x + number = number, x = ?\"\n",
    "lin_sub  = re.compile(r'^x\\-(-?\\d+)=(-?\\d+),x=\\?$') # to check if it matches \"x - number = number, x = ?\"\n",
    "lin_mul  = re.compile(r'^(\\d+)\\*x=(-?\\d+),x=\\?$') # to check if it matches \"number * x = number, x = ?\"\n",
    "\n",
    "# Definition to extract only the question part before the answer\n",
    "def extract_prompt(s):\n",
    "    # get the part before the first space (answer)\n",
    "    return s.split(' ', 1)[0]\n",
    "# Definition to check if positive response actually contain the correct answer\n",
    "def check_positive_record(rec):\n",
    "    s = rec[\"positive\"]\n",
    "    prompt = extract_prompt(s)\n",
    "\n",
    "    # to recalculate the answer given, and compare against the answer in the positive response\n",
    "    m = arith_re.match(prompt)\n",
    "    if m: # if it is arithmetic equation\n",
    "        a, op, b = int(m.group(1)), m.group(2), int(m.group(3))\n",
    "        ans = a+b if op=='+' else a-b if op=='-' else a*b if op=='*' else (a//b)\n",
    "    else: # if it is linear equation\n",
    "        m1 = lin_add.match(prompt)\n",
    "        m2 = lin_sub.match(prompt)\n",
    "        m3 = lin_mul.match(prompt)\n",
    "        if m1:\n",
    "            b, c = int(m1.group(1)), int(m1.group(2))\n",
    "            ans = c - b\n",
    "        elif m2:\n",
    "            b, c = int(m2.group(1)), int(m2.group(2))\n",
    "            ans = c + b\n",
    "        elif m3:\n",
    "            k, c = int(m3.group(1)), int(m3.group(2))\n",
    "            ans = c // k\n",
    "        else:\n",
    "            # Other types are Phase 2, so skip\n",
    "            return True\n",
    "\n",
    "    return f\" {ans}\" in s  # check to see if actual answer exists in the resposne given\n",
    "\n",
    "# Report any bad examples\n",
    "bad = [i for i,rec in enumerate(data) if not check_positive_record(rec)] # this will identify any problematic records\n",
    "# hence print out pass rate of the data pair dataset generated\n",
    "print(f\"Positives passing numeric sanity: {len(data)-len(bad)}/{len(data)}\")\n",
    "# if the pass rate check is less than 100%, it will print out the first 5 bad indices for debugging purposes\n",
    "if bad[:5]: print(\"First 5 bad indices:\", bad[:5])\n",
    "\n",
    "\n",
    "### Save Generated Dataset to File ###\n",
    "import os\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "path = \"./data/pos_neg_pairs.json\"\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864c1ca",
   "metadata": {},
   "source": [
    "### <u>Step 5.2: Load Data from './data/pos_neg_pairs.json'</u>\n",
    "\n",
    "In this step, we will:\n",
    "- 1. Open the json file and read the dataset file that contains the positive-negative pairs.\n",
    "- 2. Parse the json file contents by converting it from the json format to python's list of dictionaries.\n",
    "- 3. Store all training examples in variable 'lines'.\n",
    "    - Each element in lines is a dictionary with 2 keys:\n",
    "        - 'negative': Incorrect or undesirable response; \"x-7=1,x=? Sorry, I do not know!\"\n",
    "        - 'positive': Correct or human preferred response; \"x-7=1,x=? The answer is 8 because 1+7 equals 8.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t_DC5LEkOpuN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1760974317909,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "t_DC5LEkOpuN",
    "outputId": "fd9f29c2-7a09-4cd9-fdc7-2045fb687632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configurator.py  data  dpo  model.py  __pycache__  README.md  sft\n",
      "Loaded 250000 training pairs\n"
     ]
    }
   ],
   "source": [
    "# Check the working directory that we are currently in\n",
    "!ls\n",
    "\n",
    "# Load data from ./data/pos_neg_pairs.json\n",
    "path = \"./data/pos_neg_pairs.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = json.load(f)\n",
    "\n",
    "# Check the number of data pairs in the dataset that has been loaded into 'line'\n",
    "print(f\"Loaded {len(lines)} training pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {
    "id": "c2e5f81f"
   },
   "source": [
    "## <u>Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)</u>\n",
    "\n",
    "This step is crucial as it will configure the optimisation algorithm with selective weight decay and a learning rate schedule with warmup, that is required for stable and effective training of the DPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c400f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1760974326312,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "df0c400f",
    "outputId": "e840bc5f-9d08-49fc-ade6-1bcaf27b7838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and scheduler ready.\n"
     ]
    }
   ],
   "source": [
    "# Ensure that all necessary imports are done\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "\"\"\" Referencing to Step 2.2.2 - Hyperparameter Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 2e-4\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.1\n",
    "top_k = None\n",
    "\"\"\"\n",
    "\n",
    "# calculation of batches per epoch; to determine training schedule\n",
    "# since our dataset has 250,000 pairs, this will be 250,000 // 64 ≈ 3906 steps\n",
    "# additionally, having the max() function, also ensures that it will prevent any cases where steps_per_epoch equals to 0\n",
    "steps_per_epoch = max(1, len(lines) // batch_size) \n",
    "\n",
    "# calculation of total gradient updates across all epochs; this is important for learning rate scheduling as well as warmup calculations\n",
    "# since our number of batches/steps per epoch is ≈ 3906 steps, and epochs = 10, the total training steps would be 3906 * 10\n",
    "total_training_steps = steps_per_epoch * epochs\n",
    "\n",
    "# calculation of warmup; this is done by gradually increasing the learning rate from near-zero value to base_lr value, to allow the model to ease into optimisation and prevent early training explosions that can come about with too much data overload at once in the beginning of training\n",
    "# first 5% of training steps is around 1953 steps (5% * 39060)\n",
    "# 5% will allow the training of the model to stabalise, as it is short enough to not waaste time, but sufficiently long enough to stabalise\n",
    "warmup_iters = int(0.05 * total_training_steps)\n",
    "\n",
    "# calculation of total steps needed for the learning rate decay schedule\n",
    "# this is set to decay throughout the entire training of the model\n",
    "# according to best practices, the learning rate decays throughout training, which can be seen from the GPT2 and GPT3 model, so we will follow that approach\n",
    "lr_decay_iters = total_training_steps\n",
    "\n",
    "# calculation of minimum learning rate\n",
    "# this is set to 5% of base learning rate as we do not want the training to completely stop, this also makes the model make smaller adjustments towards the end\n",
    "# 5% * 2e-1 = 0.00001\n",
    "min_lr = base_lr * 0.05\n",
    "\n",
    "# AdamW with proper decay\n",
    "# for this step, we are following GPT2/GPT3's training best practices\n",
    "decay, no_decay = [], [] # no decay = bias and normalisation layers, decay = all other weights\n",
    "for n, p in gpt.named_parameters(): # n = param name, p = param tensor containing the weights\n",
    "    # if params is not updated / trainable during training, skip; we only want to apply weight decay to trainable parameters\n",
    "    if not p.requires_grad: \n",
    "        continue\n",
    "    # if there is bias or layernorm, append to no decay\n",
    "    if n.endswith(\"bias\") or \"ln\" in n.lower() or \"layernorm\" in n.lower(): \n",
    "        no_decay.append(p) # bias and normalisation layers\n",
    "    # if there consists of all other weights, append to decay\n",
    "    else:\n",
    "        decay.append(p) # all other weights\n",
    "\n",
    "# move model to device before creating the optimiser as the optimiser needs to know the parameter device locations\n",
    "gpt.to(device)\n",
    "\n",
    "# Now we move onto creating the AdamW Optimiser\n",
    "optimizer = AdamW(\n",
    "    [{\"params\": decay, \"weight_decay\": 0.1}, # has regularisation, if there is decay; this prevents catastrophic forgetting and maintains the pre-trained knowledge\n",
    "     {\"params\": no_decay, \"weight_decay\": 0.0}], # has no regularisation, if there is no decay\n",
    "    # for the adamw hyperparameters\n",
    "    # for the learning rate, we set it to the same as the base_lr, for faster DPO model adaptation\n",
    "    lr=base_lr,\n",
    "    # for the betas, we follow the standard for transformer models, such as the GPT model\n",
    "    # for the first moment, beta1, it captures gradient direction, so we set 90% of weights to historical gradients, and 10% to current gradients\n",
    "    # for the second moment, beta2, it captures gradient scale, so we set beta2 to 0.999, which is a very slow decay, thus, providing stable and adaptive learning rates\n",
    "    betas=(0.9, 0.999),\n",
    "    # for the epsilon, we set it to the standard 1e-8, which is the standard across deep learning models, since it is small enough to be invisible in normal computation, but large enough to be meaningful in any floating-point arithmetics\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# Learning Rate Schedule Function\n",
    "def lr_lambda(step):\n",
    "    # linear warmup - this stabalises early training, by linearly increasing the learning rate from 0 to base_lr\n",
    "    if step < warmup_iters: \n",
    "        return step / max(1, warmup_iters)\n",
    "    \n",
    "    # cosine decay - this ensures smooth and gradual decrease and maintains a higher learning rate for a longer period of time\n",
    "    # cosine decay is also widely used in models like GPT3, which is why we decided to follow this method\n",
    "    progress = (step - warmup_iters) / max(1, lr_decay_iters - warmup_iters)\n",
    "    return max(min_lr / base_lr, 0.5 * (1.0 + math.cos(math.pi * progress))) # this prevents the learning rate from going below the min_lr\n",
    "\n",
    "# creation of the scheduler\n",
    "# the LambdaLR scheduler will apply the custom learning rate function as defined above which will be called after each optimiser step\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# validate that all of the above steps ran smoothly with no errors\n",
    "print(\"Optimizer and scheduler ready.\")\n",
    "\n",
    "\n",
    "### Tokeniser ###\n",
    "# ensure package imported\n",
    "import pickle\n",
    "# load the vocabulary - char to ID and ID to char mappings\n",
    "with open(\"sft/meta.pkl\",\"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "# get Null chars as padding and falls back to 0 if '\\x00' is not in the vocabulary - consistent padding to ensure predictable behaviour\n",
    "PAD_ID = stoi.get('\\x00', 0)\n",
    "\n",
    "# thus now the encode function can handle out of vocabulary chars gracefully by mapping to the PAD_ID as defined above instead of crashing and giving us an error message\n",
    "def encode(s: str):\n",
    "    return [stoi.get(ch, PAD_ID) for ch in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return ''.join(itos[i] for i in ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {
    "id": "52b66199"
   },
   "source": [
    "## Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0a848",
   "metadata": {},
   "source": [
    "### <u>Step 7.1: Training the new model</u>\n",
    "\n",
    "In order to train the model, we would need to compare the _**likelihood of a correct vs wrong answer**_ through the model's mean sequence log-probability at each training step. \n",
    "\n",
    "From there, we can then apply pairwise preference loss. This helps the model learn by replicating preference patterns found in the comparison data we have provided by pushing it to _**score positives > negatives**_. \n",
    "\n",
    "This formula is denoted as follows : \n",
    "<p style=\"text-align:center;\"><em>L<sub>pref</sub> ​= −E[ log σ( <math><mfrac><mn>1</mn><mi>β</mi></mfrac></math>​( logp<sub>θ</sub>​(pos) − logp<sub>θ</sub>​(neg) ) ) ]</em></p>\n",
    "\n",
    "_**E**_ refers to the expectation over the current training batch, and _**β**_ refers to the temperature/margin scale (in this case it is inversed).\n",
    "\n",
    "For optimization, we would use AdamW with stable learning-rate schedule _*(refer to Step 6)*_. \n",
    "\n",
    "For each training step, we would: \n",
    "\n",
    "1. Zero the gradients from previous step \n",
    "2. Backpropagating the loss to compute the new gradients\n",
    "3. Gradient clipping the norm to be 1.0 or below to make it stable\n",
    "4. Use the gradients stored to update the parameters in AdamW\n",
    "5. Adjust the learning rate for the next step based on schedule\n",
    "\n",
    "As we are interested in the training curves, we recorded the loss per step and current LR. This is used for us to plot a graph later on and verify that the schedule and loss behave as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ebeb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757999,
     "status": "ok",
     "timestamp": 1760976086603,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "1d4ebeb4",
    "outputId": "6d1c2944-77fe-40e5-c925-62a7a3cfd0fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.20it/s, loss=0.0206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.22it/s, loss=0.0172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:56, 22.17it/s, loss=0.0147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.23it/s, loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.24it/s, loss=0.0143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.27it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.23it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.26it/s, loss=0.014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.24it/s, loss=0.0138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3906it [02:55, 22.26it/s, loss=0.0139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to dpo/dpo.pt\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "\n",
    "# Store the values so that we can look at learning rate and loss later in graph plot\n",
    "loss_history = []\n",
    "lr_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        '''\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples:\n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob\n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "        # ...\n",
    "        ###########################################################\n",
    "        '''\n",
    "\n",
    "        # Move tensors to device (makes sure that its the same)\n",
    "        neg_tensor = neg_tensor.to(device)\n",
    "        pos_tensor = pos_tensor.to(device)\n",
    "\n",
    "        # Compute log-probabilities with the helper provided in sample notebook\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "\n",
    "        ### DPO pairwise preference loss ###\n",
    "        # What we are looking here for is for pos_logprob > neg_logprob. In this case, the margin is pos - neg.\n",
    "        # When the margin is small or negative, this formula helps to penalise the model\n",
    "        #\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "\n",
    "        ### Record loss per step and current LR ###\n",
    "        # We want to look at learning rate and loss n plot graph, hence we will be storing loss_val and current_lr as the model runs\n",
    "        loss_val = loss.item()\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_history.append(current_lr)\n",
    "\n",
    "        ### Optimising the model ###\n",
    "        # 1. Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # 2. Backpropagate to compute the new gradients\n",
    "        loss.backward()\n",
    "        # 3. Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n",
    "        # 4. Apply AdamW optimiser\n",
    "        optimizer.step()\n",
    "        # 5. Apply learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # This is used for visual purposes to display the most recent loss, so that we can view in real-time the change in loss as we train the model\n",
    "        loss_val = loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss_val})\n",
    "\n",
    "    # Once an epoch has passed, we save it as a checkpoint to dpo.pt\n",
    "    ckpt_path = f\"dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2349091",
   "metadata": {},
   "source": [
    "### <u>Step 7.2: Plotting</u>\n",
    "\n",
    "Once our model has finished training, we plot a graph to verify that the schedule and loss behave as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w31jzWWZPHEv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1760976117496,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "w31jzWWZPHEv",
    "outputId": "778f26b8-7f8e-4d46-c5f6-dfa82a7fd658"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training loss per step\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_history, label=\"Loss per batch\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot learning rate schedule\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(loss_history, label=\"Learning Rate\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate vs Step\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {
    "id": "48b7f2ab"
   },
   "source": [
    "## Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6fa88",
   "metadata": {},
   "source": [
    "#### For Testing, we looped through the following steps : \n",
    "1. Encode the prompt\n",
    "2. Convert to tensor of token IDs\n",
    "3. Run through model\n",
    "4. Decode the output\n",
    "5. Print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09027262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18670,
     "status": "ok",
     "timestamp": 1760976329753,
     "user": {
      "displayName": "Jordan Choi",
      "userId": "16495390488022399426"
     },
     "user_tz": -480
    },
    "id": "09027262",
    "outputId": "d0fd2f1e-a10f-4992-8b52-fc7705f0e232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT : 17+19=?\n",
      "OUTPUT : 17+19=? The answer is 36 because 17+19 equals 36.\n",
      "------------------------------\n",
      "PROMPT : 3*17=?\n",
      "OUTPUT : 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "------------------------------\n",
      "PROMPT : 72/4=?\n",
      "OUTPUT : 72/4=? The answer is 7 because 72/4 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 72-x=34,x=?\n",
      "OUTPUT : 72-x=34,x=? The answer is 16 because 34-7 equals 6.\n",
      "------------------------------\n",
      "PROMPT : x*11=44,x=?\n",
      "OUTPUT : x*11=44,x=? The answer is 4 because 44/11 equals 4.\n",
      "------------------------------\n",
      "PROMPT : 3*17=?\n",
      "OUTPUT : 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "------------------------------\n",
      "PROMPT : 72/4=?\n",
      "OUTPUT : 72/4=? The answer is 7 because 72/4 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 72-x=34,x=?\n",
      "OUTPUT : 72-x=34,x=? The answer is 16 because 34-7 equals 6.\n",
      "------------------------------\n",
      "PROMPT : 8+12=?\n",
      "OUTPUT : 8+12=? The answer is 20 because 8+12 equals 20.\n",
      "------------------------------\n",
      "PROMPT : 50-23=?\n",
      "OUTPUT : 50-23=? The answer is 27 because 50-23 equals 27.\n",
      "------------------------------\n",
      "PROMPT : 9*8=?\n",
      "OUTPUT : 9*8=? The answer is 72 because 9*8 equals 72.\n",
      "------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x+15=30,x=?\n",
      "OUTPUT : x+15=30,x=? The answer is 15 because 30-15 equals 15.\n",
      "------------------------------\n",
      "PROMPT : 7*x=49,x=?\n",
      "OUTPUT : 7*x=49,x=? The answer is 7 because 49/7 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 5*6=?\n",
      "OUTPUT : 5*6=? The answer is 30 because 5*6 equals 30.\n",
      "------------------------------\n",
      "PROMPT : 100/25=?\n",
      "OUTPUT : 100/25=? The answer is 5 because 10/12 equals 55.\n",
      "------------------------------\n",
      "PROMPT : x-7=15,x=?\n",
      "OUTPUT : x-7=15,x=? The answer is 11 because 15+7 equals 11.\n",
      "------------------------------\n",
      "PROMPT : 9+x=20,x=?\n",
      "OUTPUT : 9+x=20,x=? The answer is 11 because 20-9 equals 11.\n",
      "------------------------------\n",
      "PROMPT : 12*12=?\n",
      "OUTPUT : 12*12=? The answer is 144 because 12*12 equals 144.\n",
      "------------------------------\n",
      "PROMPT : 64/8=?\n",
      "OUTPUT : 64/8=? The answer is 3 because 64/8 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x+9=18,x=?\n",
      "OUTPUT : x+9=18,x=? The answer is 9 because 18-9 equals 9.\n",
      "------------------------------\n",
      "PROMPT : 10*10=?\n",
      "OUTPUT : 10*10=? The answer is 100 because 10*10 equals 100.\n",
      "------------------------------\n",
      "PROMPT : 45/5=?\n",
      "OUTPUT : 45/5=? The answer is 9 because 45/5 equals 9.\n",
      "------------------------------\n",
      "PROMPT : x-4=10,x=?\n",
      "OUTPUT : x-4=10,x=? The answer is 14 because 10+4 equals 14.\n",
      "------------------------------\n",
      "PROMPT : x+7=14,x=?\n",
      "OUTPUT : x+7=14,x=? The answer is 7 because 14-7 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 6*7=?\n",
      "OUTPUT : 6*7=? The answer is 42 because 6*7 equals 42.\n",
      "------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x-3=12,x=?\n",
      "OUTPUT : x-3=12,x=? The answer is 15 because 12+3 equals 15.\n",
      "------------------------------\n",
      "PROMPT : 8+x=15,x=?\n",
      "OUTPUT : 8+x=15,x=? The answer is 7 because 15-8 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 14*14=?\n",
      "OUTPUT : 14*14=? The answer is 196 because 14*14 equals 196.\n",
      "------------------------------\n",
      "PROMPT : 72/8=?\n",
      "OUTPUT : 72/8=? The answer is 4 because 72/8 equals 4.\n",
      "------------------------------\n",
      "PROMPT : x+8=16,x=?\n",
      "OUTPUT : x+8=16,x=? The answer is 8 because 16-8 equals 8.\n",
      "------------------------------\n",
      "PROMPT : 9-3=?\n",
      "OUTPUT : 9-3=? The answer is 6 because 9-3 equals 6.\n",
      "------------------------------\n",
      "PROMPT : 15*3=?\n",
      "OUTPUT : 15*3=? The answer is 45 because 15*3 equals 45.\n",
      "------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x-5=10,x=?\n",
      "OUTPUT : x-5=10,x=? The answer is 15 because 10+5 equals 15.\n",
      "------------------------------\n",
      "PROMPT : x+6=13,x=?\n",
      "OUTPUT : x+6=13,x=? The answer is 7 because 13-6 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 11*11=?\n",
      "OUTPUT : 11*11=? The answer is 121 because 11*11 equals 121.\n",
      "------------------------------\n",
      "PROMPT : 56/7=?\n",
      "OUTPUT : 56/7=? The answer is 8 because 56/7 equals 8.\n",
      "------------------------------\n",
      "PROMPT : x+10=20,x=?\n",
      "OUTPUT : x+10=20,x=? The answer is 10 because 20-10 equals 10.\n",
      "------------------------------\n",
      "PROMPT : 7*8=?\n",
      "OUTPUT : 7*8=? The answer is 56 because 7*8 equals 56.\n",
      "------------------------------\n",
      "PROMPT : 90/9=?\n",
      "OUTPUT : 90/9=? The answer is 2 because 90/9 equals 2.\n",
      "------------------------------\n",
      "PROMPT : x-2=8,x=?\n",
      "OUTPUT : x-2=8,x=? The answer is 10 because 8+2 equals 10.\n",
      "------------------------------\n",
      "PROMPT : x+5=12,x=?\n",
      "OUTPUT : x+5=12,x=? The answer is 7 because 12-5 equals 7.\n",
      "------------------------------\n",
      "PROMPT : 13*13=?\n",
      "OUTPUT : 13*13=? The answer is 169 because 13*13 equals 169.\n",
      "------------------------------\n",
      "PROMPT : 72/6=?\n",
      "OUTPUT : 72/6=? The answer is 2 because 72/6 equals 2.\n",
      "------------------------------\n",
      "PROMPT : x-6=14,x=?\n",
      "OUTPUT : x-6=14,x=? The answer is 20 because 14+6 equals 20.\n",
      "------------------------------\n",
      "PROMPT : 4*9=?\n",
      "OUTPUT : 4*9=? The answer is 36 because 4*9 equals 36.\n",
      "------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x+4=10,x=?\n",
      "OUTPUT : x+4=10,x=? The answer is 6 because 10-4 equals 6.\n",
      "------------------------------\n",
      "PROMPT : 8*8=?\n",
      "OUTPUT : 8*8=? The answer is 64 because 8*8 equals 64.\n",
      "------------------------------\n",
      "PROMPT : 60/5=?\n",
      "OUTPUT : 60/5=? The answer is 10 because 60/5 equals 10.\n",
      "------------------------------\n",
      "PROMPT : x-1=9,x=?\n",
      "OUTPUT : x-1=9,x=? The answer is 10 because 9+1 equals 10.\n",
      "------------------------------\n",
      "PROMPT : x+3=11,x=?\n",
      "OUTPUT : x+3=11,x=? The answer is 8 because 11-3 equals 8.\n",
      "------------------------------\n",
      "PROMPT : 9*9=?\n",
      "OUTPUT : 9*9=? The answer is 81 because 9*9 equals 81.\n",
      "------------------------------\n",
      "PROMPT : 49/7=?\n",
      "OUTPUT : 49/7=? The answer is 7 because 49/7 equals 7.\n",
      "------------------------------\n",
      "PROMPT : x-4=11,x=?\n",
      "OUTPUT : x-4=11,x=? The answer is 15 because 11+4 equals 15.\n",
      "------------------------------\n",
      "PROMPT : 6*6=?\n",
      "OUTPUT : 6*6=? The answer is 36 because 6*6 equals 36.\n",
      "------------------------------\n",
      "PROMPT : 100/10=?\n",
      "OUTPUT : 100/10=? The answer is 10 because 10/11 equals 1.\n",
      "------------------------------\n",
      "PROMPT : x+2=12,x=?\n",
      "OUTPUT : x+2=12,x=? The answer is 10 because 12-2 equals 10.\n",
      "------------------------------\n",
      "PROMPT : 15-7=?\n",
      "OUTPUT : 15-7=? The answer is 8 because 15-7 equals 8.\n",
      "------------------------------\n",
      "PROMPT : 8*7=?\n",
      "OUTPUT : 8*7=? The answer is 56 because 8*7 equals 56.\n",
      "------------------------------\n",
      "PROMPT : 72/9=?\n",
      "OUTPUT : 72/9=? The answer is 2 because 72/9 equals 2.\n",
      "------------------------------\n",
      "PROMPT : x-8=12,x=?\n",
      "OUTPUT : x-8=12,x=? The answer is 20 because 12+8 equals 20.\n",
      "------------------------------\n",
      "PROMPT : x+1=10,x=?\n",
      "OUTPUT : x+1=10,x=? The answer is 9 because 10-1 equals 9.\n",
      "------------------------------\n",
      "PROMPT : 14*12=?\n",
      "OUTPUT : 14*12=? The answer is 168 because 14*12 equals 168.\n",
      "------------------------------\n",
      "PROMPT : 54/6=?\n",
      "OUTPUT : 54/6=? The answer is 4 because 54/6 equals 4.\n",
      "------------------------------\n",
      "PROMPT : x-9=11,x=?\n",
      "OUTPUT : x-9=11,x=? The answer is 20 because 11+9 equals 20.\n",
      "------------------------------\n",
      "PROMPT : 5*8=?\n",
      "OUTPUT : 5*8=? The answer is 40 because 5*8 equals 40.\n",
      "------------------------------\n",
      "PROMPT : 81/9=?\n",
      "OUTPUT : 81/9=? The answer is 3 because 81/9 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x+12=20,x=?\n",
      "OUTPUT : x+12=20,x=? The answer is 8 because 20-12 equals 8.\n",
      "------------------------------\n",
      "PROMPT : 7*9=?\n",
      "OUTPUT : 7*9=? The answer is 63 because 7*9 equals 63.\n",
      "------------------------------\n",
      "PROMPT : 80/8=?\n",
      "OUTPUT : 80/8=? The answer is 5 because 80/8 equals 5.\n",
      "------------------------------\n",
      "PROMPT : x-7=13,x=?\n",
      "OUTPUT : x-7=13,x=? The answer is 20 because 13+7 equals 20.\n",
      "------------------------------\n",
      "PROMPT : x+11=22,x=?\n",
      "OUTPUT : x+11=22,x=? The answer is 11 because 22-11 equals 11.\n",
      "------------------------------\n",
      "PROMPT : 12*11=?\n",
      "OUTPUT : 12*11=? The answer is 132 because 12*11 equals 132.\n",
      "------------------------------\n",
      "PROMPT : 63/7=?\n",
      "OUTPUT : 63/7=? The answer is 3 because 63/7 equals 3.\n",
      "------------------------------\n",
      "PROMPT : x-5=15,x=?\n",
      "OUTPUT : x-5=15,x=? The answer is 20 because 15+5 equals 20.\n",
      "------------------------------\n",
      "PROMPT : 9*7=?\n",
      "OUTPUT : 9*7=? The answer is 63 because 9*7 equals 63.\n",
      "------------------------------\n",
      "PROMPT : 90/10=?\n",
      "OUTPUT : 90/10=? The answer is 9 because 90/10 equals 9.\n",
      "------------------------------\n",
      "PROMPT : x+15=25,x=?\n",
      "OUTPUT : x+15=25,x=? The answer is 10 because 25-15 equals 10.\n",
      "------------------------------\n",
      "PROMPT : 6*8=?\n",
      "OUTPUT : 6*8=? The answer is 48 because 6*8 equals 48.\n",
      "------------------------------\n",
      "PROMPT : 72/8=?\n",
      "OUTPUT : 72/8=? The answer is 4 because 72/8 equals 4.\n",
      "------------------------------\n",
      "PROMPT : x-4=16,x=?\n",
      "OUTPUT : x-4=16,x=? The answer is 20 because 16+4 equals 20.\n",
      "------------------------------\n",
      "PROMPT : x+14=28,x=?\n",
      "OUTPUT : x+14=28,x=? The answer is 14 because 28-14 equals 14.\n",
      "------------------------------\n",
      "PROMPT : 11*12=?\n",
      "OUTPUT : 11*12=? The answer is 132 because 11*12 equals 132.\n",
      "------------------------------\n",
      "PROMPT : 48/6=?\n",
      "OUTPUT : 48/6=? The answer is 8 because 48/6 equals 8.\n",
      "------------------------------\n",
      "PROMPT : x-3=13,x=?\n",
      "OUTPUT : x-3=13,x=? The answer is 16 because 13+3 equals 16.\n",
      "------------------------------\n",
      "PROMPT : 4*8=?\n",
      "OUTPUT : 4*8=? The answer is 32 because 4*8 equals 32.\n",
      "------------------------------\n",
      "PROMPT : 56/8=?\n",
      "OUTPUT : 56/8=? The answer is 2 because 56/8 equals 2.\n",
      "------------------------------\n",
      "PROMPT : x+13=26,x=?\n",
      "OUTPUT : x+13=26,x=? The answer is 13 because 26-13 equals 13.\n",
      "------------------------------\n",
      "PROMPT : 10*12=?\n",
      "OUTPUT : 10*12=? The answer is 120 because 10*12 equals 120.\n",
      "------------------------------\n",
      "PROMPT : 70/7=?\n",
      "OUTPUT : 70/7=? The answer is 2 because 70/7 equals 2.\n",
      "------------------------------\n",
      "PROMPT : x-6=18,x=?\n",
      "OUTPUT : x-6=18,x=? The answer is 20 because 18+6 equals 20.\n",
      "------------------------------\n",
      "PROMPT : x+9=19,x=?\n",
      "OUTPUT : x+9=19,x=? The answer is 10 because 19-9 equals 10.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "\n",
    "# Added 'cpu' as potential device as most of us are using Macbook Apple Silicon\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt = GPT(gptconf).to(device)\n",
    "\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"8+12=?\", \"50-23=?\", \"9*8=?\", \"81/9=?\", \"x+15=30,x=?\", \"7*x=49,x=?\", \"5*6=?\", \"100/25=?\", \"x-7=15,x=?\", \"9+x=20,x=?\", \"12*12=?\", \"64/8=?\", \"x+9=18,x=?\", \"10*10=?\", \"45/5=?\", \"x-4=10,x=?\", \"x+7=14,x=?\", \"6*7=?\", \"81/9=?\", \"x-3=12,x=?\", \"8+x=15,x=?\", \"14*14=?\", \"72/8=?\", \"x+8=16,x=?\", \"9-3=?\", \"15*3=?\", \"81/9=?\", \"x-5=10,x=?\", \"x+6=13,x=?\", \"11*11=?\", \"56/7=?\", \"x+10=20,x=?\", \"7*8=?\", \"90/9=?\", \"x-2=8,x=?\", \"x+5=12,x=?\", \"13*13=?\", \"72/6=?\", \"x-6=14,x=?\", \"4*9=?\", \"81/9=?\", \"x+4=10,x=?\", \"8*8=?\", \"60/5=?\", \"x-1=9,x=?\", \"x+3=11,x=?\", \"9*9=?\", \"49/7=?\", \"x-4=11,x=?\", \"6*6=?\", \"100/10=?\", \"x+2=12,x=?\", \"15-7=?\", \"8*7=?\", \"72/9=?\", \"x-8=12,x=?\", \"x+1=10,x=?\", \"14*12=?\", \"54/6=?\", \"x-9=11,x=?\", \"5*8=?\", \"81/9=?\", \"x+12=20,x=?\", \"7*9=?\", \"80/8=?\", \"x-7=13,x=?\", \"x+11=22,x=?\", \"12*11=?\", \"63/7=?\", \"x-5=15,x=?\", \"9*7=?\", \"90/10=?\", \"x+15=25,x=?\", \"6*8=?\", \"72/8=?\", \"x-4=16,x=?\", \"x+14=28,x=?\", \"11*12=?\", \"48/6=?\", \"x-3=13,x=?\", \"4*8=?\", \"56/8=?\", \"x+13=26,x=?\", \"10*12=?\", \"70/7=?\", \"x-6=18,x=?\", \"x+9=19,x=?\"]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        '''\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        '''\n",
    "\n",
    "        # Converts the prompt to a tensor of token IDs based on our tokeniser\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, :]\n",
    "        \n",
    "        # Run through the trained model to generate the resultant tensor values\n",
    "        y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature,top_k=top_k)\n",
    "\n",
    "        # We will take the tensor of token IDs and flatten it out to prep it for decoding\n",
    "        ans = y[0].reshape(-1).tolist()\n",
    "\n",
    "        # The output will be the decoded token IDs\n",
    "        output = decode(ans)\n",
    "\n",
    "        # Print out the prompt and output respectively\n",
    "        print(f\"PROMPT : {prompt}\")\n",
    "        print(f\"OUTPUT : {output}\")\n",
    "        print(\"-\" * 60)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df813a13",
   "metadata": {},
   "source": [
    "#### For quick testing with text file, follow the steps and just run the cell below :\n",
    "1. Ensure the test case are in a .txt file in the following format :<br>\n",
    "<b>[\"17+19=?\", \"3*17=?\", ... ,\"72/4=?\"]</b><br><br>\n",
    "2. Edit the string_path variable<br><br>\n",
    "3. Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be modified\n",
    "string_path = \"\" \n",
    "\n",
    "# No editing required\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "from model import GPT, GPTConfig\n",
    "\n",
    "max_new_tokens = 200\n",
    "temperature = 0.05\n",
    "top_k = None\n",
    "\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ckpt_path = \"dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).to(device)\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.eval()\n",
    "\n",
    "try:\n",
    "    with open(string_path, \"r\") as file:\n",
    "        priv_set = file.read()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for prompt in priv_set:\n",
    "            prompt_ids = encode(prompt)\n",
    "            x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, :]\n",
    "            y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature,top_k=top_k)\n",
    "            ans = y[0].reshape(-1).tolist()\n",
    "            output = decode(ans)\n",
    "\n",
    "            print(f\"PROMPT : {prompt}\")\n",
    "            print(f\"OUTPUT : {output}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error : File not found\")\n",
    "except IOError as e:\n",
    "    print(f\"Error : {e}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
